{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE: Maximum Likelihood Estimation (Estimación de máxima verosimilitud)\n",
    "\n",
    "Es una técnica que nos permite _estimar_ densidades de probabilidad de un conjunto de datos.\n",
    "\n",
    "Su objetivo es buscar la forma más óptima de ajustar una distribución a los datos.\n",
    "\n",
    "Puntos clave:\n",
    "\n",
    "- Escoger una distribución: Teniendo solo una muestra de datos.\n",
    "\n",
    "- Escoger los parámetros de la distribución: Que ajusten mejor la distribución a los datos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un problema fundamental del MLE es que no tenemos la población total de los datos, sino solamente una muestra de ellos.\n",
    "\n",
    "\n",
    "MLE es un problema de optimización."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión lineal con MLE\n",
    "\n",
    "$$ y = \\underbrace{m}_{\\text{pendiente}} x + \\overbrace{b}^{\\text{intercepto}} = \\underbrace{b_0}_{\\text{weight}} x + \\overbrace{b_1}^{\\text{bias}} $$\n",
    "<br>\n",
    "\n",
    "La ecuación de la recta de toda la vida se utiliza en Machine Learning, algunos parámetros tendrán un nombre distinto, pero es la misma ecuación.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$P(y|x) \\rightarrow max\\sum_i{log P(y_i | x_i; \\overbrace{h}^{\\text{modelo}})} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En regresión lineal se utiliza un método conocido como 'mínimos cuadrados'.\n",
    "Este método busca minimizar la suma de las diferencias entre el valor de $y (y_i)$  y el valor teórico de $y (mx_i + b)$, elevando el resultado al cuadrado para así eliminar cualquier valor negativo. Podemos escribir la formula así:\n",
    "\n",
    "### Mínimos cuadrados: $$ \\boxed{\\min \\sum_{i}{(y_i -mx_i + b)}^2} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La estimación de máxima verosimilitud es equivalente al método de los mínimos cuadrados que vemos arriba."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\max{ \\left\\{ \\displaystyle\\sum_{i} {\\ln P (y_i, x_i, h)} \\right\\} } = \\max{ \\left\\{ \\displaystyle\\sum_{i} {\\ln \\Bigg(   \\frac{1}{\\sigma\\sqrt{2\\pi}} \\LARGE{e}^{-\\frac{1}{2} \\Big( \\frac{y_i - (mx_i + b) }{\\sigma} \\Big)^2 } \\Bigg)  } \\right\\} }$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicando la propiedad de los logaritmos: \n",
    "##### $$\\ln(ab) = \\ln(a) + \\ln(b)$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\max{ \\left\\{ \\LARGE{\\displaystyle\\sum_{i}} { \\LARGE{\\ln} \\Bigg(   \\frac{1}{\\sigma\\sqrt{2\\pi}} \\Bigg)}    +   \\LARGE{\\displaystyle\\sum_{i}} { \\LARGE{\\ln} \\Bigg(    \\LARGE{e}^{-\\frac{1}{2} \\Big( \\frac{y_i - (mx_i + b) }{\\sigma} \\Big)^2 } \\Bigg)}   \\right\\} }$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el primer término es una constante, podemos descartarlo.\n",
    "\n",
    "Para el segundo término debemos recordar que la función $\\ln$ es la inversa de la función $\\exp$ y ya que que $f^{-1}(f(x)) = x $ nuestra ecuación quedaría de la siguiente manera:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\max{ \\left\\{  \\displaystyle\\sum_{i} \\Bigg( -\\frac{1}{2} \\bigg( \\frac{y_i - (mx_i + b) }{\\sigma} \\bigg)^2  \\Bigg)  \\right\\} }$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerando el máximo negativo como el mínimo y moviendo moviendo las constantes, quedaríamos con la ecuación:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$ \\boxed{ \\min{ \\left\\{  \\frac{1}{2\\sigma^2} \\sum_{i} \\big(  y_i - (mx_i + b ) \\big)^2 \\right\\} }  }$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo que vemos que el problema de estimación de máxima verosimilitud también consiste en calcular el mínimo de la suma de los errores cuadrados, exactamente igual que en los mínimos cuadrados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$ \\boxed{\\min \\sum_{i}{(y_i -mx_i + b)}^2} $$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
